{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spring 2019 CX4240 Homework 4\n",
    "\n",
    "## Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: April 17, Wednesday, 11:59 pm\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged, but each student must write his own answers and explicitly mention any collaborators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Descriptive Questions (45pts)\n",
    "\n",
    "\n",
    "You could type with Latex (directly type in the markdown). Hand-writting is also acceptable. If you hand write, you could scan your answer and include it as a picture. If you want to add any picture to your answer, use this syntax $\"<img src=\"\" style=\"width: 500px;\"/>\"$ to include them within your ipython notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 SVM Concepts (20pts)\n",
    "\n",
    "(1) What linear function is used by a SVM for classification? How is an input vector $x_i$ (instance) assigned to the positive or negative class? (5pts)\n",
    "\n",
    "(A) The linear function used is $w^T x + b$. An input vector is assigned to the positive class if $f(x_i) < 0$ and to the negative class if $f(x_i) >= 0$ \n",
    "\n",
    "(2) If the training examples are linearly separable, how many decision boundaries can separate positive from negative data points? Which decision boundary does the SVM algorithm calculate? Why? (5pts)\n",
    "\n",
    "(A) If training is linearly separable, there are infinite number of decision boundaries that can separate negative from positive data points. The SVM calculates the decision boundary with the largest margin to increase confidence in correct assginment of data points.\n",
    "\n",
    "(3) What is the margin? Which are the equations of the two margin hyperplans $H_+$ and $H_-$ ? (5pts)\n",
    "\n",
    "(A) Margin is te distance from decision boundary to closest data point and that same distance to the other side of the boundary. <br>\n",
    "$H_+ = w^T x + b = 1$  <br>\n",
    "$H_- = w^T x + b = -1$\n",
    "\n",
    "(4) Summarize the main advantages and limitations of SVM. (5pts)\n",
    "\n",
    "Advatage: The regularization parameters lets users avoid overfitting, the kernel can be adjusted to user preference, is efficient to solve depending on methods, and work well with unstructured data \n",
    "\n",
    "Limitations: Deciding on a kernel function is hard and can be sensitive, training time can be long, need a model per class in multiclass problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 SVM Kernel (25pts)\n",
    "\n",
    "Suppose we have a dataset in 1-dimensional space which consists of 3 data points ${−1, 0, 1}$ with the positive label and 3 data points ${−3, −2, 2}$ with the negative label.\n",
    "\n",
    "(1) Find a feature map $(R1 → R2)$, which will map the original 1-dimensional data points to the 2-dimensional feature space so that the positive samples and the negative samples are linearly separable with each other. Draw the dataset after mapping in the 2-dimensional space. (10pts)\n",
    "\n",
    "same plot below\n",
    "\n",
    "(2) In your plot above, draw the decision boundary given by hard-margin linear SVM. Mark the corresponding support vectors. (5pts)\n",
    "\n",
    "<img src=\"Capture.PNG\" style=\"width: 500px;\"/>\n",
    "\n",
    "(3) For the feature map you use, what is the corresponding kernel $K(x_1,x_2)$? (10pts)\n",
    "\n",
    "The kernel used is \n",
    "$K(x_1,x_2) = \\phi(x_1)^T \\phi(x_2) = [x_1,x_1^2]^T [x_2,x_2^2] = x_1 x_2 + x_1^2 x_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Coding Questions (55pts + extra 10 bonus points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement a neural network that can be used to classify the images. \n",
    "\n",
    "Make sure the packages imported below are installed. These packages should be enough for you to finish this homework.\n",
    "\n",
    "**You don't need to modify the block below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some abstract classes defined for neural network layers. \n",
    "\n",
    "The neural networks we are talking about here consist of several stacked 'layers'. Each layer is an operation that processes the output from previous layer, and feed the output to next layer.\n",
    "\n",
    "Here are several common functions. It is ok if it doesn't look familiar for now, we will get into them step by step. \n",
    "\n",
    "(1) forward: corresponds to the 'forward pass' in the class notes.\n",
    "\n",
    "(2) backward_input: taking derivatives with respect to some inputs.\n",
    "\n",
    "(3) backward_param: taking derivatives with respect to the parameters $\\theta$\n",
    "\n",
    "**You don't need to modify the block below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = np.float32\n",
    "\n",
    "class Layer(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def forward(self, h_in):\n",
    "        raise NotImplementedError\n",
    "    @abstractmethod\n",
    "    def backward_input(self, h_in, h_out, d_hout):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ParamLayer(Layer):\n",
    "    def __init__(self):\n",
    "        self.params = dict()\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward_param(self, h_in, h_out, d_hout):\n",
    "        raise NotImplementedError    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are some utility functions. \n",
    "\n",
    "How to initialize the weights in the neural network is an interesting research topic, since different initializations will probably lead to very different final results. Here we just use one type of initialization which is commonly used in the literature.\n",
    "\n",
    "If you are interested in more details, it is called glorot_uniform, which is used to initialize the weights of the fully connected layers. See [this paper](http://proceedings.mlr.press/v9/glorot10a.html).\n",
    "\n",
    "**You don't need to modify the block below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(out_size, in_size):\n",
    "    limit = np.sqrt(6.0 / (in_size + out_size))\n",
    "    return np.random.uniform(-limit, limit, size=[out_size, in_size]).astype(DTYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the architecture\n",
    "\n",
    "Let's start from the picture we have seen in the class. It is a neural network that contains one input layer, one hidden layer, and one output layer.\n",
    "\n",
    "The weights between input and first hidden layer is called the first fully connected layer (FCLayer), since it connects all neurons between these two layers. So as for the second FCLayer.\n",
    "\n",
    "After the FCLayer, the sigmoid function we see in the class is called activation functin, where we call it SigmoidLayer in this homework for consistency. \n",
    "\n",
    "![title](nn.png)\n",
    "\n",
    "\n",
    "Below we first introduce the task, and then start with the implementation of the objective function, and next implement the forward and backward pass for the FCLayer and SigmoidLayer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to classify the hand written digits from zero to nine (totally 10 digits).\n",
    "\n",
    "Let's load the MNIST dataset, and display some of the training images to get a better idea of what we are going to do. For more information about MNIST and its history, you can read [this](http://yann.lecun.com/exdb/mnist/) if you are interested.\n",
    "\n",
    "**You don't need to modify the block below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='bytes')\n",
    "\n",
    "train_imgs, train_labels = train_set\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "# plot several images\n",
    "for i in range(20):\n",
    "    ax = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
    "    img = np.reshape(train_imgs[i], (28, 28))\n",
    "    ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is of size 28x28=784. We flat the image and use this 784-dimension vector as the feature representation for each image. In total we have 50k images for training, 10k images for validation and another 10k images for test.\n",
    "\n",
    "**You don't need to modify the block below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images (50000, 784)\n",
      "validation images (10000, 784)\n",
      "test images (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print('train images', train_set[0].shape)\n",
    "print('validation images', valid_set[0].shape)\n",
    "print('test images', test_set[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean Square Error layer (14pts)\n",
    "\n",
    "Our objective is to minimize the MSE, i.e.,:\n",
    "\n",
    "$$E(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2} \\|\\vec{y_i} - \\hat{y}(x_i; \\theta)\\|^2$$\n",
    "\n",
    "Where $y_i$ is the true label, and $\\hat{y}$ is our prediction. \n",
    "\n",
    "In class we have learned the case where $y \\in \\mathbb{R}$ is a scalar. In this homework, we make a bit of extension to that, in which $y \\in \\mathbb{R}^{10}$ is a 10-dimensional vector. The reason is, we want to use this neural network to classify the 10 types of digits.\n",
    "\n",
    "We use one-hot representation for the true label. For example, if the label of $i$-th instance is class 4, then the one-hot representation would be $y_i = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]$. \n",
    "\n",
    "**Note that MSE is not a standard loss function for multi-class classification. We use this in the homework simply because it is simpler to calculate the gradient.**\n",
    "\n",
    "Below we have a helper function that converts the class label into 10-dimensional one-hot vector.\n",
    "\n",
    "**You don't need to modify the block below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(labels):\n",
    "    one_hot = np.zeros((len(labels), 10), dtype=DTYPE)\n",
    "    one_hot[range(len(labels)), labels] = 1.0\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation: Mean Square Error calculation and derivatives.\n",
    "\n",
    "Now it's time to get your hands dirty. You are asked to implement the two functions below.\n",
    "\n",
    "(1) forward (**7pts**): given the true labels and prediction $\\hat{y_i}$, first convert the labels into one-hot representation $y_i$, then calculate the error $\\frac{1}{2}\\|y_i - \\hat{y_i}\\|^2$. Note that there will be N instances fed into this function, so you need to return the average MSE. \n",
    "\n",
    "(2) backward_input (**7pts**): you are asked to implement the derivatives with respect to your prediction, \n",
    "$$\\frac{\\partial E_i}{\\partial \\hat{y}_i} = y_i - \\hat{y}_i$$. Do this for all the N instances in this batch, and return a matrix with size $N * 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquareError(Layer):\n",
    "    \n",
    "    def forward(self, labels, predict):\n",
    "        one_hot_label = get_onehot(labels)\n",
    "        N = predict.shape[0]\n",
    "        loss = []\n",
    "        for i in range(N):\n",
    "            err = 0.5 * np.linalg.norm(predict[i,:] - one_hot_label[i,:])**2 \n",
    "            loss = np.append(loss, err)\n",
    "        loss = np.mean(loss)\n",
    "        return loss\n",
    "\n",
    "    def backward_input(self, labels, predict):\n",
    "        one_hot_label = get_onehot(labels)\n",
    "        deriv = predict - one_hot_label\n",
    "        return deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fully connected layer (21pts)\n",
    "\n",
    "Below you are asked to implement the forward, backward_input and backward_param functions. \n",
    "\n",
    "Note that the multiplication '$*$' below means matrix multiplication (i.e., np.dot)\n",
    "\n",
    "(1) forward (**7pts**): let's copy the architecture figure below:\n",
    "\n",
    "<img src=\"fc.png\" style=\"width: 150px;\"/>\n",
    "\n",
    "\n",
    "In the above example, we have $x$ of dimension 1, and hidden layer of dimension 2, then we'll have 1 * 2 + 2 parameters for $\\theta$ in this layer. \n",
    "\n",
    "Let's redefine the notation a bit: $\\theta_{1, 0} = \\theta_{1}$, $\\theta_{2, 0} = \\theta_{2}$, $\\theta_{1, 1} = \\theta_{3}$, $\\theta_{2, 1} = \\theta_{4}$\n",
    "\n",
    "If we have $x$ of dimension 784, and hidden layer of size 1024, then we'll have 784 * 1024 + 1024 parameters for $\\theta$ in this layer. The $i$-th neuron $u_{i} = \\sum_{j=1}^{784} x_j * \\theta_{i, j} + \\theta_{i, 0}$.  \n",
    "\n",
    "To make the notation simple, let $W \\in \\mathbb{R}^{1024 * 784}$ be the weight matrix, where $W_{i, j} = \\theta_{i, j}$. And let $b \\in \\mathbb{R}^{1024}$ be the bias vector, where $b_i = \\theta_{i, 0}$. Then we can rewrite the above summation in the matrix format:\n",
    "\n",
    "$$\\vec{u} = \\vec{x} * W^\\top + b$$\n",
    "\n",
    "**Keep the above equation in mind, as it is need for both forward and backward implementations.**\n",
    "\n",
    "You are supposed to write the above matrix multiplication inside the forward function.\n",
    "\n",
    "(2) backward_input (**7pts**): we want the derivatives w.r.t input $\\vec{x}$, i.e., $\\frac{\\partial E(\\theta)}{\\partial \\vec{x}}$. \n",
    "\n",
    "use the chain rule, $\\frac{\\partial E(\\theta)}{\\partial \\vec{x}} = \\frac{\\partial E(\\theta)}{\\partial \\vec{u}} * \\frac{\\partial \\vec{u}}{\\partial \\vec{x}}$. \n",
    "\n",
    "$\\frac{\\partial E(\\theta)}{\\partial \\vec{u}}$ will be provided as an argument to this function, so you are only asked to implement $\\frac{\\partial \\vec{u}}{\\partial \\vec{x}} and get the chain rule done.\n",
    "\n",
    "(3) backward_param (**7pts**):  we want the derivatives w.r.t parameter $W, b$, i.e., $\\frac{\\partial E(\\theta)}{\\partial W}$ and $\\frac{\\partial E(\\theta)}{\\partial b}$. \n",
    "\n",
    "use the chain rule again, $\\frac{\\partial E(\\theta)}{\\partial W} = \\frac{\\partial E(\\theta)}{\\partial \\vec{u}} * \\frac{\\partial \\vec{u}}{\\partial W}$. \n",
    "\n",
    "and \n",
    "\n",
    "$\\frac{\\partial E(\\theta)}{\\partial b} = \\frac{\\partial E(\\theta)}{\\partial \\vec{u}} * \\frac{\\partial \\vec{u}}{\\partial b}$.\n",
    "\n",
    "Again, the $\\frac{\\partial E(\\theta)}{\\partial \\vec{u}}$ will be provided to this function, so you are only asked to implement $\\frac{\\partial \\vec{u}}{\\partial W}$ and $\\frac{\\partial \\vec{u}}{\\partial b}$, to get the chain rule done.\n",
    "\n",
    "**You don't need to modify __init__**\n",
    "\n",
    "#### Implementation: You are asked to implement forward, backward_input, backward_param. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(ParamLayer):\n",
    "    def __init__(self, out_size, in_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            out_size: output dimension\n",
    "            in_size: input dimension\n",
    "        \"\"\" \n",
    "        super(FCLayer, self).__init__()\n",
    "        self.out_size = out_size\n",
    "        self.in_size = in_size\n",
    "        self.params[\"weight\"] = weight_init(out_size, in_size)\n",
    "        self.params[\"bias\"] = np.zeros(out_size, dtype=DTYPE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: array of size N x in_size, where N is the batch size\n",
    "        Return:\n",
    "            u: array of size N x out_size\n",
    "        \"\"\"\n",
    "        Wt = np.transpose(self.params[\"weight\"])\n",
    "        u = np.dot(x, Wt) + self.params[\"bias\"]\n",
    "        return u \n",
    "\n",
    "    def backward_input(self, x, u, de_du):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: array of size N x in_size, where N is the batch size\n",
    "            u: array of size N x out_size, \n",
    "            de_du: \\frac{\\partial E(\\theta)}{\\partial \\vec{u}} in the above equation\n",
    "        Return:\n",
    "            de_dx: \\frac{\\partial E(\\theta)}{\\partial \\vec{x}} in the above equation\n",
    "        \"\"\"\n",
    "        de_dx = np.dot(de_du, self.params[\"weight\"])\n",
    "        return de_dx\n",
    "\n",
    "    def backward_param(self, x, u, de_du):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: array of size N x in_size, where N is the batch size\n",
    "            u: array of size N x out_size, \n",
    "            de_du: \\frac{\\partial E(\\theta)}{\\partial \\vec{u}} in the above equation\n",
    "        Return:\n",
    "            dictionary of {\n",
    "                \"weight\": \\frac{\\partial E(\\theta)}{\\partial W} in the above equation\n",
    "                \"bias\": \\frac{\\partial E(\\theta)}{\\partial b} in the above equation\n",
    "            }\n",
    "        \"\"\"\n",
    "        N = x.shape[0]\n",
    "        de_dw = np.dot(de_du.T, x)\n",
    "        de_db = np.dot(de_du.T, np.ones(N))\n",
    "        D = {} \n",
    "        D[\"weight\"] = de_dw \n",
    "        D[\"bias\"] = de_db \n",
    "        return D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sigmoid activation layer implementation (14pts)\n",
    "\n",
    "One key component in a neural network is the nonlinear activation function. \n",
    "\n",
    "<img src=\"sigmoid.png\" style=\"width: 150px;\"/>\n",
    "\n",
    "\n",
    "**forward implementation (7pts)**\n",
    "\n",
    "$O = Sigmoid(\\mu) = \\frac{1}{1 + \\exp(-\\mu)}$\n",
    "\n",
    "**backward_input implementation (7pts)**\n",
    "Use the notes in the class to get the derivatives. \n",
    "\n",
    "$\\frac{\\partial E(\\theta)}{\\partial \\mu} = \\frac{\\partial E(\\theta)}{O} * \\frac{\\partial O}{\\partial \\mu}$.\n",
    "\n",
    "$\\frac{\\partial E(\\theta)}{O}$ will be provided as the input to the function. You are only asked to calculate \n",
    "$\\frac{\\partial O}{\\partial \\mu}$ and get the chain rule done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer(Layer):\n",
    "\n",
    "    def forward(self, u):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            u: array of size N x D, where N is the batch size\n",
    "        Return:\n",
    "            O: array of size N x D\n",
    "        \"\"\"\n",
    "        return 1/(1+np.exp(-u))\n",
    "    \n",
    "    def backward_input(self, u, O, de_dO):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            u: array of size N x D, where N is the batch size\n",
    "            O: array of size N x D, \n",
    "            de_dO: \\frac{\\partial L}{\\partial h}\n",
    "        Return:\n",
    "            de_du: \\frac{\\partial L}{\\partial x}\n",
    "        \"\"\"\n",
    "        dO_du = np.dot(O.T, 1-O)\n",
    "        de_du = np.dot(de_dO, dO_du)\n",
    "        return de_du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: ReLU (Rectifier linear unit) activation layer implementation\n",
    "\n",
    "**You will get extra 10pts bonus points if you also implement this one correctly.**\n",
    "\n",
    "Another option is Relu. In this setting, the Relu will have ~20% accuracy boost from sigmoid function, after 10 epochs of training. \n",
    "\n",
    "For each neuron $x$ in the corresponding hidden layer $h_i$, the relu function is:\n",
    "\n",
    "$Relu(x) = x$, if $x > 0$, else $0$\n",
    "\n",
    "The derivative can be calculated for all $x$ except at $x=0$, where the function is not smooth. One practice is to simply ignore the case when x=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer(Layer):\n",
    "\n",
    "    def forward(self, h_in):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h_in: array of size N x D, where N is the batch size\n",
    "        Return:\n",
    "            h_out: array of size N x D\n",
    "        \"\"\"        \n",
    "        return np.maximum(0,h_in)\n",
    "\n",
    "    def backward_input(self, h_in, h_out, d_hout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h_in: array of size N x in_size, where N is the batch size\n",
    "            h_out: array of size N x out_size, \n",
    "            d_hout: \\frac{\\partial L}{\\partial h} in the above equation\n",
    "        Return:\n",
    "            d_hin: \\frac{\\partial L}{\\partial x} in the above equation\n",
    "        \"\"\"\n",
    "        return 1. * (h_in > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter which one you implemented, choose the one that is to be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below if you use sigmoid\n",
    "ACT_FUNC = SigmoidLayer\n",
    "# uncomment below if you use relu\n",
    "#ACT_FUNC = ReLULayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Neural Network implementation (6pts)\n",
    "\n",
    "\n",
    "Now we are ready to put everything together and get our neural network assembled!\n",
    "\n",
    "We call it MLP (multi layer perception). \n",
    "\n",
    "**To make it simple, you are only required to implement the \"update\" function, which corresponds to the SGD update operation.**\n",
    "\n",
    "$$W = W - \\eta * dW$$, where $W$ is the parameter you want to update, $\\eta$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, in_dim, hidden_dims, out_dim):\n",
    "        \"\"\"Do not modify\"\"\"\n",
    "        self.act_layer = ACT_FUNC()\n",
    "        \n",
    "        dims = [in_dim] + hidden_dims + [out_dim]\n",
    "        self.fc_layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            fc_layer = FCLayer(out_size=dims[i + 1], in_size=dims[i])\n",
    "            self.fc_layers.append(fc_layer)\n",
    "        self.loss_func = MeanSquareError()\n",
    "        \n",
    "    def forward(self, img_input, img_label):\n",
    "        \"\"\"Do not modify\"\"\"\n",
    "        x = img_input\n",
    "        self.hiddens = [x]\n",
    "        for i in range(len(self.fc_layers)):\n",
    "            x = self.fc_layers[i].forward(x)\n",
    "            self.hiddens.append(x)\n",
    "            if i + 1 < len(self.fc_layers):\n",
    "                x = self.act_layer.forward(x)\n",
    "                self.hiddens.append(x)\n",
    "        logits = x\n",
    "        loss = self.loss_func.forward(img_label, logits)\n",
    "        predict = np.argmax(logits, axis=1)\n",
    "        accuracy = np.mean(predict == img_label)\n",
    "        return loss, accuracy\n",
    "\n",
    "    def backward(self, img_label):\n",
    "        \"\"\"Do not modify\"\"\"\n",
    "        grad = self.loss_func.backward_input(img_label, self.hiddens[-1])\n",
    "        idx = len(self.hiddens) - 1\n",
    "        self.layer_grads = [None] * len(self.fc_layers)\n",
    "        for i in range(len(self.fc_layers) - 1, -1, -1):\n",
    "            assert idx >= 1\n",
    "            g_param = self.fc_layers[i].backward_param(self.hiddens[idx - 1], self.hiddens[idx], grad)\n",
    "            self.layer_grads[i] = g_param\n",
    "            grad = self.fc_layers[i].backward_input(self.hiddens[idx - 1], self.hiddens[idx], grad)            \n",
    "            idx -= 1\n",
    "            if i > 0:\n",
    "                grad = self.act_layer.backward_input(self.hiddens[idx - 1], self.hiddens[idx], grad)\n",
    "                idx -= 1\n",
    "        assert idx == 0\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            learning_rate: scalar, the \\eta in above\n",
    "        \"\"\"\n",
    "        for i in range(len(self.fc_layers)):\n",
    "            grad_params = self.layer_grads[i]\n",
    "            params = self.fc_layers[i].params\n",
    "            # params is a dictionary of parameters: {'weight': np array, 'bias': np array}\n",
    "            # grad_params is a dictionary of gradients w.r.t. parameters: {'weight': np array, 'bias': np array}            \n",
    "            # TODO: update params[\"weight\"] and params[\"bias\"] with grad_params[\"weight\"] and grad_params[\"bias\"]\n",
    "            \n",
    "            params[\"weight\"] = params[\"weight\"] - learning_rate *  grad_params[\"weight\"]\n",
    "            #params[\"weight\"] -= np.dot(learning_rate, grad_params[\"weight\"])\n",
    "            params[\"bias\"] = params[\"bias\"] - learning_rate * grad_params[\"bias\"]\n",
    "            #params[\"bias\"] -= np.dot(learning_rate, grad_params[\"bias\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the MNIST, we create a neural network with 2 hidden layers, each of them has size of 1024.\n",
    "Since it is a 10-class classification problem, the neural network will have an output of size 10.\n",
    "\n",
    "To summarize, the neural network has the layer sizes of 784 -> 1024 -> 1024 -> 10\n",
    "\n",
    "**You don't need to modify the block below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = MLP(784, [1024, 1024], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it for 10 epochs (i.e., go through the training dataset for 10 times), report the validation performance after each epoch. \n",
    "\n",
    "**You don't need to modify the block below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94762d5eec64bffa8241fc5e694f8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n",
      "average loss: 0.45025395555079734\n",
      "average accuracy: 0.1064\n",
      "training epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc4a758c8064d25a6b699ddccc6b026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n",
      "average loss: 0.44993655767660734\n",
      "average accuracy: 0.1064\n"
     ]
    }
   ],
   "source": [
    "def loop_over_dataset(net, imgs, labels, is_training, batch_size=100):\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    pbar = range(0, imgs.shape[0], batch_size)\n",
    "    if is_training:\n",
    "        pbar = tqdm(pbar)\n",
    "    for i in pbar:\n",
    "        x = imgs[i : i + batch_size, :]\n",
    "        y = labels[i : i + batch_size]\n",
    "        loss, acc = net.forward(x, y)\n",
    "        if is_training:\n",
    "            net.backward(y)\n",
    "            net.update(5e-5)\n",
    "        loss_list.append(loss)\n",
    "        acc_list.append(acc)\n",
    "        if is_training:\n",
    "            pbar.set_description('loss: %.4f, acc: %.4f' % (loss, acc))\n",
    "    if not is_training:\n",
    "        print('average loss:', np.mean(loss_list))\n",
    "        print('average accuracy:', np.mean(acc_list))\n",
    "\n",
    "num_epochs = 2\n",
    "for e in range(num_epochs):\n",
    "    print('training epoch', e + 1)\n",
    "    loop_over_dataset(net, train_set[0], train_set[1], is_training=True)\n",
    "    print('validation')\n",
    "    loop_over_dataset(net, valid_set[0], valid_set[1], is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And do the evaluation on the test set. \n",
    "\n",
    "**You don't need to modify the block below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_over_dataset(net, test_set[0], test_set[1], is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
